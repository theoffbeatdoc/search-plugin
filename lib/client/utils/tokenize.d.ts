/**
 * Split a sentence to tokens, considering a sequence of consecutive Chinese words as a single token.
 *
 * @param text - Text to be tokenized.
 *
 * @returns Tokens.
 */
export declare function tokenize(text: string): string[];
